{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Reshape, Softmax, Layer, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet121, ResNet50, EfficientNetB0\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4571 images belonging to 4 classes.\n",
      "Found 1141 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=15,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\"data_large/Training\",\n",
    "                                               target_size=(224, 224),\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='categorical',\n",
    "                                               subset='training')\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(\"data_large/Training\",\n",
    "                                           target_size=(224, 224),\n",
    "                                           batch_size=32,\n",
    "                                           class_mode='categorical',\n",
    "                                           subset='validation')\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\"data_large/Testing\",\n",
    "                                             target_size=(224, 224),\n",
    "                                             batch_size=32,\n",
    "                                             class_mode='categorical',\n",
    "                                             shuffle=False)\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = train_data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'glioma': 0, 'meningioma': 1, 'notumor': 2, 'pituitary': 3}\n",
      "{'glioma': 0, 'meningioma': 1, 'notumor': 2, 'pituitary': 3}\n",
      "{'glioma': 0, 'meningioma': 1, 'notumor': 2, 'pituitary': 3}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_indices)\n",
    "print(val_data.class_indices)\n",
    "print(test_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 24s 271ms/step - loss: 2.7661 - accuracy: 0.4802 - val_loss: 9.5807 - val_accuracy: 0.2880\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 17s 229ms/step - loss: 0.9405 - accuracy: 0.6413 - val_loss: 11.1500 - val_accuracy: 0.2880\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 18s 245ms/step - loss: 0.8943 - accuracy: 0.6387 - val_loss: 7.7676 - val_accuracy: 0.2914\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 18s 243ms/step - loss: 0.8341 - accuracy: 0.6652 - val_loss: 3.6061 - val_accuracy: 0.3508\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 17s 233ms/step - loss: 0.7814 - accuracy: 0.6839 - val_loss: 5.2824 - val_accuracy: 0.2949\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 17s 233ms/step - loss: 0.7411 - accuracy: 0.6939 - val_loss: 3.9133 - val_accuracy: 0.2042\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 17s 235ms/step - loss: 0.7169 - accuracy: 0.7088 - val_loss: 2.1793 - val_accuracy: 0.3578\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 17s 235ms/step - loss: 0.6964 - accuracy: 0.7188 - val_loss: 1.3296 - val_accuracy: 0.4782\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 18s 243ms/step - loss: 0.6484 - accuracy: 0.7440 - val_loss: 1.2244 - val_accuracy: 0.4939\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 18s 248ms/step - loss: 0.6322 - accuracy: 0.7458 - val_loss: 1.1055 - val_accuracy: 0.5271\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 6.0657 - accuracy: 0.4213\n",
      "Test Accuracy: 0.4213\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "143/143 [==============================] - 41s 273ms/step - loss: 1.5473 - accuracy: 0.6165 - val_loss: 8.6996 - val_accuracy: 0.2726\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 35s 244ms/step - loss: 0.7206 - accuracy: 0.7184 - val_loss: 3.1272 - val_accuracy: 0.5188\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 34s 237ms/step - loss: 0.6224 - accuracy: 0.7626 - val_loss: 2.8683 - val_accuracy: 0.3637\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 34s 237ms/step - loss: 0.5602 - accuracy: 0.7828 - val_loss: 0.6189 - val_accuracy: 0.7748\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 35s 244ms/step - loss: 0.5131 - accuracy: 0.8025 - val_loss: 0.6905 - val_accuracy: 0.7607\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 34s 236ms/step - loss: 0.4962 - accuracy: 0.8060 - val_loss: 0.8556 - val_accuracy: 0.7616\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 34s 238ms/step - loss: 0.4566 - accuracy: 0.8280 - val_loss: 1.5465 - val_accuracy: 0.7143\n",
      "Epoch 8/10\n",
      "143/143 [==============================] - 34s 237ms/step - loss: 0.4450 - accuracy: 0.8377 - val_loss: 0.7695 - val_accuracy: 0.7616\n",
      "Epoch 9/10\n",
      "143/143 [==============================] - 33s 233ms/step - loss: 0.4155 - accuracy: 0.8414 - val_loss: 0.9938 - val_accuracy: 0.7467\n",
      "Epoch 10/10\n",
      "143/143 [==============================] - 34s 236ms/step - loss: 0.4054 - accuracy: 0.8515 - val_loss: 0.7780 - val_accuracy: 0.7713\n",
      "41/41 [==============================] - 3s 60ms/step - loss: 0.6296 - accuracy: 0.7841\n",
      "Learning Rate: 0.0001, Test Accuracy: 0.7841\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 34s 235ms/step - loss: 5.3963 - accuracy: 0.5502 - val_loss: 14.6133 - val_accuracy: 0.2682\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 34s 237ms/step - loss: 1.2286 - accuracy: 0.5506 - val_loss: 13.7201 - val_accuracy: 0.2813\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 35s 242ms/step - loss: 0.9687 - accuracy: 0.6071 - val_loss: 1.4409 - val_accuracy: 0.3865\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 36s 253ms/step - loss: 0.8913 - accuracy: 0.6340 - val_loss: 1.0827 - val_accuracy: 0.6564\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 37s 255ms/step - loss: 0.9792 - accuracy: 0.6115 - val_loss: 1.1736 - val_accuracy: 0.6696\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 37s 256ms/step - loss: 0.8040 - accuracy: 0.6644 - val_loss: 1.3014 - val_accuracy: 0.6599\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 34s 239ms/step - loss: 0.8047 - accuracy: 0.6778 - val_loss: 1.3563 - val_accuracy: 0.7485\n",
      "Epoch 8/10\n",
      "143/143 [==============================] - 33s 230ms/step - loss: 0.7628 - accuracy: 0.7064 - val_loss: 2.3880 - val_accuracy: 0.5145\n",
      "Epoch 9/10\n",
      "143/143 [==============================] - 33s 231ms/step - loss: 0.7703 - accuracy: 0.7086 - val_loss: 2.3093 - val_accuracy: 0.5933\n",
      "Epoch 10/10\n",
      "143/143 [==============================] - 33s 228ms/step - loss: 0.7186 - accuracy: 0.7047 - val_loss: 1.1888 - val_accuracy: 0.7450\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 1.1444 - accuracy: 0.7315\n",
      "Learning Rate: 0.001, Test Accuracy: 0.7315\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 34s 233ms/step - loss: 52.7030 - accuracy: 0.4625 - val_loss: 5.0031 - val_accuracy: 0.2507\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 33s 231ms/step - loss: 4.3980 - accuracy: 0.4067 - val_loss: 1.9199 - val_accuracy: 0.2770\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 33s 229ms/step - loss: 1.7402 - accuracy: 0.3855 - val_loss: 1.5486 - val_accuracy: 0.2524\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 33s 232ms/step - loss: 1.5872 - accuracy: 0.3573 - val_loss: 3.6932 - val_accuracy: 0.2927\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 33s 230ms/step - loss: 2.0718 - accuracy: 0.3702 - val_loss: 2.2989 - val_accuracy: 0.2463\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 34s 239ms/step - loss: 4.5942 - accuracy: 0.3818 - val_loss: 6.5623 - val_accuracy: 0.3339\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 33s 230ms/step - loss: 1.6105 - accuracy: 0.3894 - val_loss: 2.4795 - val_accuracy: 0.3208\n",
      "Epoch 8/10\n",
      "143/143 [==============================] - 33s 228ms/step - loss: 2.1596 - accuracy: 0.3594 - val_loss: 1.3941 - val_accuracy: 0.2559\n",
      "Epoch 9/10\n",
      "143/143 [==============================] - 33s 232ms/step - loss: 1.4976 - accuracy: 0.3290 - val_loss: 35.0723 - val_accuracy: 0.3444\n",
      "Epoch 10/10\n",
      "143/143 [==============================] - 33s 231ms/step - loss: 1.4680 - accuracy: 0.3249 - val_loss: 32.8021 - val_accuracy: 0.3269\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 20.4396 - accuracy: 0.3478\n",
      "Learning Rate: 0.01, Test Accuracy: 0.3478\n",
      "Best Learning Rate: 0.0001 with Test Accuracy: 0.7841\n"
     ]
    }
   ],
   "source": [
    "# Defining the basic model architecture\n",
    "def create_model(input_shape, num_classes, learning_rate):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Greedy Feature Selection and Hyperparameter Search\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "best_acc = 0\n",
    "best_lr = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = create_model(input_shape=input_shape, num_classes=num_classes, learning_rate=lr)\n",
    "    history = model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "    test_loss, test_acc = model.evaluate(test_data)\n",
    "    \n",
    "    print(f\"Learning Rate: {lr}, Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"Best Learning Rate: {best_lr} with Test Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 32 filters, kernel_size (3, 3), 256 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 37s 249ms/step - loss: 0.7735 - accuracy: 0.6939 - val_loss: 4.0943 - val_accuracy: 0.2550\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 34s 240ms/step - loss: 0.5971 - accuracy: 0.7709 - val_loss: 4.2274 - val_accuracy: 0.2901\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 34s 235ms/step - loss: 0.5158 - accuracy: 0.8000 - val_loss: 7.6811 - val_accuracy: 0.2621\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 34s 236ms/step - loss: 0.4521 - accuracy: 0.8333 - val_loss: 1.0321 - val_accuracy: 0.6284\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 34s 238ms/step - loss: 0.3938 - accuracy: 0.8541 - val_loss: 1.0805 - val_accuracy: 0.6801\n",
      "41/41 [==============================] - 2s 41ms/step - loss: 1.0403 - accuracy: 0.6560\n",
      "Test Accuracy: 0.6560\n",
      "Training with 32 filters, kernel_size (3, 3), 256 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 34s 237ms/step - loss: 0.7989 - accuracy: 0.6756 - val_loss: 2.1732 - val_accuracy: 0.2577\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 35s 246ms/step - loss: 0.5912 - accuracy: 0.7718 - val_loss: 4.7520 - val_accuracy: 0.2585\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 33s 233ms/step - loss: 0.5159 - accuracy: 0.8029 - val_loss: 5.3741 - val_accuracy: 0.2997\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 33s 233ms/step - loss: 0.4745 - accuracy: 0.8162 - val_loss: 1.0895 - val_accuracy: 0.6582\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 34s 237ms/step - loss: 0.4347 - accuracy: 0.8394 - val_loss: 1.2352 - val_accuracy: 0.5635\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 1.1841 - accuracy: 0.6133\n",
      "Test Accuracy: 0.6133\n",
      "Training with 32 filters, kernel_size (3, 3), 512 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 35s 240ms/step - loss: 0.7675 - accuracy: 0.6869 - val_loss: 3.4858 - val_accuracy: 0.2550\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 33s 233ms/step - loss: 0.5718 - accuracy: 0.7742 - val_loss: 11.4822 - val_accuracy: 0.2550\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 33s 232ms/step - loss: 0.4995 - accuracy: 0.8114 - val_loss: 3.8701 - val_accuracy: 0.3839\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 33s 231ms/step - loss: 0.4611 - accuracy: 0.8252 - val_loss: 1.9178 - val_accuracy: 0.4724\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 33s 229ms/step - loss: 0.3979 - accuracy: 0.8504 - val_loss: 2.9516 - val_accuracy: 0.4417\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 2.8526 - accuracy: 0.4561\n",
      "Test Accuracy: 0.4561\n",
      "Training with 32 filters, kernel_size (3, 3), 512 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 34s 233ms/step - loss: 0.7616 - accuracy: 0.6922 - val_loss: 4.9569 - val_accuracy: 0.2550\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 33s 229ms/step - loss: 0.6019 - accuracy: 0.7648 - val_loss: 7.9639 - val_accuracy: 0.2568\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 33s 233ms/step - loss: 0.5322 - accuracy: 0.7926 - val_loss: 6.8485 - val_accuracy: 0.2936\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 33s 233ms/step - loss: 0.4740 - accuracy: 0.8210 - val_loss: 6.9190 - val_accuracy: 0.3050\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 33s 231ms/step - loss: 0.4285 - accuracy: 0.8390 - val_loss: 2.0044 - val_accuracy: 0.4706\n",
      "41/41 [==============================] - 2s 39ms/step - loss: 1.9900 - accuracy: 0.4775\n",
      "Test Accuracy: 0.4775\n",
      "Training with 32 filters, kernel_size (5, 5), 256 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 41s 258ms/step - loss: 0.7737 - accuracy: 0.6918 - val_loss: 2.1414 - val_accuracy: 0.3646\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 33s 231ms/step - loss: 0.6332 - accuracy: 0.7532 - val_loss: 3.0504 - val_accuracy: 0.3427\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 34s 234ms/step - loss: 0.5473 - accuracy: 0.7948 - val_loss: 1.9571 - val_accuracy: 0.4759\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 34s 238ms/step - loss: 0.5017 - accuracy: 0.8035 - val_loss: 1.0888 - val_accuracy: 0.6862\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 34s 234ms/step - loss: 0.4511 - accuracy: 0.8217 - val_loss: 0.9103 - val_accuracy: 0.6152\n",
      "41/41 [==============================] - 2s 60ms/step - loss: 0.8929 - accuracy: 0.5889\n",
      "Test Accuracy: 0.5889\n",
      "Training with 32 filters, kernel_size (5, 5), 256 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 35s 243ms/step - loss: 0.7970 - accuracy: 0.6872 - val_loss: 3.0922 - val_accuracy: 0.3225\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 36s 248ms/step - loss: 0.6193 - accuracy: 0.7609 - val_loss: 6.5013 - val_accuracy: 0.2585\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 34s 236ms/step - loss: 0.5378 - accuracy: 0.7893 - val_loss: 7.6263 - val_accuracy: 0.2638\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 34s 237ms/step - loss: 0.4980 - accuracy: 0.8003 - val_loss: 2.6940 - val_accuracy: 0.5188\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 34s 235ms/step - loss: 0.4662 - accuracy: 0.8171 - val_loss: 1.4818 - val_accuracy: 0.5478\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 1.3687 - accuracy: 0.5919\n",
      "Test Accuracy: 0.5919\n",
      "Training with 32 filters, kernel_size (5, 5), 512 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 35s 237ms/step - loss: 0.7939 - accuracy: 0.6760 - val_loss: 2.6226 - val_accuracy: 0.2997\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 36s 249ms/step - loss: 0.6149 - accuracy: 0.7532 - val_loss: 3.4483 - val_accuracy: 0.3146\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 34s 239ms/step - loss: 0.5395 - accuracy: 0.7882 - val_loss: 4.3714 - val_accuracy: 0.3287\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 36s 249ms/step - loss: 0.5093 - accuracy: 0.7924 - val_loss: 3.0874 - val_accuracy: 0.3164\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 36s 247ms/step - loss: 0.4543 - accuracy: 0.8263 - val_loss: 0.8990 - val_accuracy: 0.7257\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.8164 - accuracy: 0.7361\n",
      "Test Accuracy: 0.7361\n",
      "Training with 32 filters, kernel_size (5, 5), 512 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 35s 241ms/step - loss: 0.7383 - accuracy: 0.7031 - val_loss: 6.2507 - val_accuracy: 0.2770\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 35s 244ms/step - loss: 0.5647 - accuracy: 0.7797 - val_loss: 4.8890 - val_accuracy: 0.3550\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 35s 242ms/step - loss: 0.5087 - accuracy: 0.7974 - val_loss: 4.8253 - val_accuracy: 0.4987\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 34s 236ms/step - loss: 0.4783 - accuracy: 0.8149 - val_loss: 4.3041 - val_accuracy: 0.4943\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 34s 238ms/step - loss: 0.4261 - accuracy: 0.8420 - val_loss: 1.1120 - val_accuracy: 0.6722\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 1.1031 - accuracy: 0.6728\n",
      "Test Accuracy: 0.6728\n",
      "Training with 64 filters, kernel_size (3, 3), 256 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 60s 345ms/step - loss: 0.7782 - accuracy: 0.6885 - val_loss: 2.6870 - val_accuracy: 0.2612\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 38s 267ms/step - loss: 0.5997 - accuracy: 0.7677 - val_loss: 5.4853 - val_accuracy: 0.2550\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 38s 262ms/step - loss: 0.4870 - accuracy: 0.8114 - val_loss: 6.4347 - val_accuracy: 0.3690\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 38s 262ms/step - loss: 0.4596 - accuracy: 0.8228 - val_loss: 3.4623 - val_accuracy: 0.4224\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 37s 260ms/step - loss: 0.4042 - accuracy: 0.8536 - val_loss: 1.1002 - val_accuracy: 0.6521\n",
      "41/41 [==============================] - 3s 80ms/step - loss: 1.3303 - accuracy: 0.6423\n",
      "Test Accuracy: 0.6423\n",
      "Training with 64 filters, kernel_size (3, 3), 256 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 38s 264ms/step - loss: 0.7834 - accuracy: 0.6863 - val_loss: 4.8503 - val_accuracy: 0.2550\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 38s 262ms/step - loss: 0.6193 - accuracy: 0.7572 - val_loss: 8.6689 - val_accuracy: 0.2550\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 38s 262ms/step - loss: 0.5359 - accuracy: 0.7948 - val_loss: 5.5322 - val_accuracy: 0.3365\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 38s 262ms/step - loss: 0.4749 - accuracy: 0.8204 - val_loss: 3.2286 - val_accuracy: 0.5872\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 38s 261ms/step - loss: 0.4380 - accuracy: 0.8357 - val_loss: 0.6874 - val_accuracy: 0.7686\n",
      "41/41 [==============================] - 2s 46ms/step - loss: 0.6963 - accuracy: 0.7468\n",
      "Test Accuracy: 0.7468\n",
      "Training with 64 filters, kernel_size (3, 3), 512 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 38s 263ms/step - loss: 0.7743 - accuracy: 0.6841 - val_loss: 3.0394 - val_accuracy: 0.2550\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 38s 262ms/step - loss: 0.6079 - accuracy: 0.7563 - val_loss: 6.0221 - val_accuracy: 0.2612\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 38s 262ms/step - loss: 0.5392 - accuracy: 0.7968 - val_loss: 6.1461 - val_accuracy: 0.2831\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 37s 261ms/step - loss: 0.4366 - accuracy: 0.8383 - val_loss: 3.0725 - val_accuracy: 0.4224\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 37s 259ms/step - loss: 0.3895 - accuracy: 0.8547 - val_loss: 1.7769 - val_accuracy: 0.5793\n",
      "41/41 [==============================] - 2s 46ms/step - loss: 1.8319 - accuracy: 0.5591\n",
      "Test Accuracy: 0.5591\n",
      "Training with 64 filters, kernel_size (3, 3), 512 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 38s 263ms/step - loss: 0.7868 - accuracy: 0.6791 - val_loss: 5.1759 - val_accuracy: 0.2550\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 38s 261ms/step - loss: 0.6022 - accuracy: 0.7602 - val_loss: 9.1523 - val_accuracy: 0.2550\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 38s 261ms/step - loss: 0.5315 - accuracy: 0.7972 - val_loss: 8.3743 - val_accuracy: 0.2585\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 38s 261ms/step - loss: 0.4499 - accuracy: 0.8261 - val_loss: 3.9489 - val_accuracy: 0.3891\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 37s 261ms/step - loss: 0.4258 - accuracy: 0.8379 - val_loss: 0.9337 - val_accuracy: 0.7143\n",
      "41/41 [==============================] - 2s 45ms/step - loss: 1.0461 - accuracy: 0.6682\n",
      "Test Accuracy: 0.6682\n",
      "Training with 64 filters, kernel_size (5, 5), 256 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 78s 440ms/step - loss: 0.7662 - accuracy: 0.6992 - val_loss: 4.3918 - val_accuracy: 0.2559\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 49s 339ms/step - loss: 0.6013 - accuracy: 0.7602 - val_loss: 5.9081 - val_accuracy: 0.2971\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 48s 337ms/step - loss: 0.5354 - accuracy: 0.7959 - val_loss: 3.7781 - val_accuracy: 0.3883\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 49s 338ms/step - loss: 0.4827 - accuracy: 0.8184 - val_loss: 1.2402 - val_accuracy: 0.5898\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 49s 339ms/step - loss: 0.4612 - accuracy: 0.8243 - val_loss: 3.8114 - val_accuracy: 0.3909\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 3.5763 - accuracy: 0.4043\n",
      "Test Accuracy: 0.4043\n",
      "Training with 64 filters, kernel_size (5, 5), 256 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 50s 343ms/step - loss: 0.7622 - accuracy: 0.6998 - val_loss: 6.4971 - val_accuracy: 0.2550\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 49s 342ms/step - loss: 0.6013 - accuracy: 0.7578 - val_loss: 8.8844 - val_accuracy: 0.2594\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 49s 342ms/step - loss: 0.5343 - accuracy: 0.7909 - val_loss: 2.3695 - val_accuracy: 0.4084\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 49s 342ms/step - loss: 0.5019 - accuracy: 0.8079 - val_loss: 1.4480 - val_accuracy: 0.5706\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 49s 339ms/step - loss: 0.4686 - accuracy: 0.8237 - val_loss: 3.7370 - val_accuracy: 0.3856\n",
      "41/41 [==============================] - 2s 51ms/step - loss: 3.5573 - accuracy: 0.3852\n",
      "Test Accuracy: 0.3852\n",
      "Training with 64 filters, kernel_size (5, 5), 512 dense units, dropout 0.3\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 50s 342ms/step - loss: 0.7572 - accuracy: 0.6959 - val_loss: 3.6988 - val_accuracy: 0.2585\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 49s 341ms/step - loss: 0.5980 - accuracy: 0.7644 - val_loss: 3.4698 - val_accuracy: 0.3953\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 49s 339ms/step - loss: 0.5205 - accuracy: 0.7904 - val_loss: 4.0861 - val_accuracy: 0.4549\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 49s 338ms/step - loss: 0.5043 - accuracy: 0.8005 - val_loss: 2.2392 - val_accuracy: 0.4996\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 49s 340ms/step - loss: 0.4645 - accuracy: 0.8250 - val_loss: 1.3090 - val_accuracy: 0.5618\n",
      "41/41 [==============================] - 2s 52ms/step - loss: 1.2660 - accuracy: 0.5828\n",
      "Test Accuracy: 0.5828\n",
      "Training with 64 filters, kernel_size (5, 5), 512 dense units, dropout 0.5\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 50s 344ms/step - loss: 0.7823 - accuracy: 0.6861 - val_loss: 2.1734 - val_accuracy: 0.3699\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 49s 342ms/step - loss: 0.5933 - accuracy: 0.7703 - val_loss: 7.2059 - val_accuracy: 0.2954\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 49s 341ms/step - loss: 0.5424 - accuracy: 0.7874 - val_loss: 6.5821 - val_accuracy: 0.3103\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 49s 340ms/step - loss: 0.4742 - accuracy: 0.8110 - val_loss: 4.5304 - val_accuracy: 0.3865\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 49s 338ms/step - loss: 0.4392 - accuracy: 0.8261 - val_loss: 2.1114 - val_accuracy: 0.5872\n",
      "41/41 [==============================] - 2s 51ms/step - loss: 1.6215 - accuracy: 0.6072\n",
      "Test Accuracy: 0.6072\n",
      "Training with 128 filters, kernel_size (3, 3), 256 dense units, dropout 0.3\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Jayan\\AppData\\Local\\Temp\\ipykernel_18244\\2894627025.py\", line 46, in <module>\n      model.fit(train_data, epochs=5, validation_data=val_data)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3'\nOOM when allocating tensor with shape[32,128,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_166880]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m filters, kernel_size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_units\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dense units, dropout \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model_varying(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, filters\u001b[38;5;241m=\u001b[39mfilters, kernel_size\u001b[38;5;241m=\u001b[39mkernel_size, dense_units\u001b[38;5;241m=\u001b[39mdense_units, dropout_rate\u001b[38;5;241m=\u001b[39mdropout_rate)\n\u001b[1;32m---> 46\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_data)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Jayan\\AppData\\Local\\Temp\\ipykernel_18244\\2894627025.py\", line 46, in <module>\n      model.fit(train_data, epochs=5, validation_data=val_data)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\Jayan\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3'\nOOM when allocating tensor with shape[32,128,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_166880]"
     ]
    }
   ],
   "source": [
    "# Function to create a model with varying architecture\n",
    "def create_model_varying(input_shape, num_classes, filters, kernel_size, dense_units, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Conv2D(filters*2, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Conv2D(filters*4, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Varying Architecture\n",
    "filters_options = [32, 64, 128]\n",
    "kernel_sizes = [(3, 3), (5, 5)]\n",
    "dense_units_options = [256, 512]\n",
    "dropout_options = [0.3, 0.5]\n",
    "best_acc = 0\n",
    "best_config = {}\n",
    "\n",
    "# Perform Greedy Feature Selection\n",
    "for filters in filters_options:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        for dense_units in dense_units_options:\n",
    "            for dropout_rate in dropout_options:\n",
    "                print(f\"Training with {filters} filters, kernel_size {kernel_size}, {dense_units} dense units, dropout {dropout_rate}\")\n",
    "                model = create_model_varying(input_shape=(224, 224, 3), num_classes=4, filters=filters, kernel_size=kernel_size, dense_units=dense_units, dropout_rate=dropout_rate)\n",
    "                model.fit(train_data, epochs=5, validation_data=val_data)\n",
    "                test_loss, test_acc = model.evaluate(test_data)\n",
    "                \n",
    "                print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "                \n",
    "                if test_acc > best_acc:\n",
    "                    best_acc = test_acc\n",
    "                    best_config = {\n",
    "                        \"filters\": filters,\n",
    "                        \"kernel_size\": kernel_size,\n",
    "                        \"dense_units\": dense_units,\n",
    "                        \"dropout_rate\": dropout_rate\n",
    "                    }\n",
    "                \n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "print(f\"Best Configuration: {best_config}\")\n",
    "print(f\"Best Test Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a model with varying hyperparameters\n",
    "def create_model_with_hyperparameters(input_shape, num_classes, learning_rate, batch_size, filters, kernel_size, dense_units, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Conv2D(filters*2, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Conv2D(filters*4, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameter options\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [32, 64, 128]\n",
    "filters_options = [32, 64]\n",
    "kernel_sizes = [(3, 3), (5, 5)]\n",
    "dense_units_options = [256, 512]\n",
    "dropout_options = [0.3, 0.5]\n",
    "\n",
    "best_acc = 0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Perform Greedy Hyperparameter Search\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for filters in filters_options:\n",
    "            for kernel_size in kernel_sizes:\n",
    "                for dense_units in dense_units_options:\n",
    "                    for dropout_rate in dropout_options:\n",
    "                        print(f\"Training with lr={lr}, batch_size={batch_size}, {filters} filters, kernel_size={kernel_size}, {dense_units} dense units, dropout={dropout_rate}\")\n",
    "                        \n",
    "                        # Create the model with the current hyperparameters\n",
    "                        model = create_model_with_hyperparameters(input_shape=(224, 224, 3), num_classes=10, learning_rate=lr, batch_size=batch_size, filters=filters, kernel_size=kernel_size, dense_units=dense_units, dropout_rate=dropout_rate)\n",
    "                        \n",
    "                        model.fit(train_data, epochs=5, batch_size=batch_size, validation_data=val_data)\n",
    "                        test_loss, test_acc = model.evaluate(test_data)\n",
    "                        \n",
    "                        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "                        \n",
    "                        if test_acc > best_acc:\n",
    "                            best_acc = test_acc\n",
    "                            best_hyperparameters = {\n",
    "                                \"learning_rate\": lr,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"filters\": filters,\n",
    "                                \"kernel_size\": kernel_size,\n",
    "                                \"dense_units\": dense_units,\n",
    "                                \"dropout_rate\": dropout_rate\n",
    "                            }\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_hyperparameters}\")\n",
    "print(f\"Best Test Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Modify model to output features from the last convolutional layer\n",
    "def create_feature_extractor(input_shape):\n",
    "    model = create_model(input_shape, num_classes=10)  # Modify num_classes according to your dataset\n",
    "    feature_extractor = Model(inputs=model.input, outputs=model.layers[-6].output)  # Extract features before the Dense layer\n",
    "    return feature_extractor\n",
    "\n",
    "# Train the original model\n",
    "input_shape = (224, 224, 3)  # Modify this according to your input image dimensions\n",
    "num_classes = 10  # Modify according to the number of classes in your dataset\n",
    "\n",
    "# Assuming train_data, val_data, test_data are already defined\n",
    "# Example: train_data = (images_train, labels_train), val_data = (images_val, labels_val)\n",
    "model = create_model(input_shape, num_classes)\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Feature extraction\n",
    "feature_extractor = create_feature_extractor(input_shape)\n",
    "features = feature_extractor.predict(train_data[0])  # Extract features from train_data\n",
    "\n",
    "# Flatten the features (if necessary)\n",
    "features_flattened = features.reshape(features.shape[0], -1)  # Flatten the features to 2D\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "n_clusters = 5  # You can adjust the number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "cluster_labels = kmeans.fit_predict(features_flattened)\n",
    "\n",
    "# Print the clustering results\n",
    "print(\"Cluster labels for the first 10 samples:\", cluster_labels[:10])\n",
    "\n",
    "# Evaluate the clustering using silhouette score (higher is better)\n",
    "silhouette_avg = silhouette_score(features_flattened, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 18s 237ms/step - loss: 1.3057 - accuracy: 0.3592 - val_loss: 1.3261 - val_accuracy: 0.3857\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 17s 235ms/step - loss: 1.1756 - accuracy: 0.4697 - val_loss: 1.4444 - val_accuracy: 0.2897\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 17s 239ms/step - loss: 1.1158 - accuracy: 0.5007 - val_loss: 1.4028 - val_accuracy: 0.3019\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 17s 236ms/step - loss: 1.0367 - accuracy: 0.5472 - val_loss: 1.5175 - val_accuracy: 0.3281\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 17s 240ms/step - loss: 0.9680 - accuracy: 0.5812 - val_loss: 1.4198 - val_accuracy: 0.3892\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 18s 250ms/step - loss: 0.9843 - accuracy: 0.5655 - val_loss: 1.3011 - val_accuracy: 0.3979\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 19s 263ms/step - loss: 0.9016 - accuracy: 0.6160 - val_loss: 1.4520 - val_accuracy: 0.4328\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 18s 255ms/step - loss: 0.8238 - accuracy: 0.6604 - val_loss: 1.4008 - val_accuracy: 0.4398\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 19s 266ms/step - loss: 0.8145 - accuracy: 0.6735 - val_loss: 1.2150 - val_accuracy: 0.4712\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 19s 258ms/step - loss: 0.7825 - accuracy: 0.6822 - val_loss: 1.2104 - val_accuracy: 0.4852\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 4.2151 - accuracy: 0.3071\n",
      "Test Accuracy: 0.3071\n"
     ]
    }
   ],
   "source": [
    "#Hybrid\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.query_dense(inputs)\n",
    "        K = self.key_dense(inputs)\n",
    "        V = self.value_dense(inputs)\n",
    "\n",
    "        attention_scores = tf.matmul(Q, K, transpose_b=True)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "\n",
    "        return tf.matmul(attention_weights, V)\n",
    "    \n",
    "def build_hybrid_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Reshape((1, 128))(x)\n",
    "\n",
    "    x = SelfAttention(128)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = build_hybrid_model(input_shape, num_classes)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 26s 290ms/step - loss: 0.9109 - accuracy: 0.6374 - val_loss: 0.7716 - val_accuracy: 0.6736\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 19s 259ms/step - loss: 0.4819 - accuracy: 0.8241 - val_loss: 0.5768 - val_accuracy: 0.7696\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 20s 270ms/step - loss: 0.3704 - accuracy: 0.8694 - val_loss: 0.4970 - val_accuracy: 0.8150\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 19s 265ms/step - loss: 0.2938 - accuracy: 0.8938 - val_loss: 0.3825 - val_accuracy: 0.8656\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 19s 257ms/step - loss: 0.2380 - accuracy: 0.9208 - val_loss: 0.3974 - val_accuracy: 0.8569\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 18s 253ms/step - loss: 0.2238 - accuracy: 0.9116 - val_loss: 0.4078 - val_accuracy: 0.8569\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 19s 258ms/step - loss: 0.1691 - accuracy: 0.9434 - val_loss: 0.4149 - val_accuracy: 0.8586\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 19s 263ms/step - loss: 0.1537 - accuracy: 0.9456 - val_loss: 0.3810 - val_accuracy: 0.8726\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 19s 260ms/step - loss: 0.1196 - accuracy: 0.9604 - val_loss: 0.3684 - val_accuracy: 0.8726\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 19s 256ms/step - loss: 0.1133 - accuracy: 0.9639 - val_loss: 0.3645 - val_accuracy: 0.8813\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 1.2591 - accuracy: 0.7310\n",
      "Test Accuracy: 0.7310\n"
     ]
    }
   ],
   "source": [
    "#DenseNet121\n",
    "\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "for layer in base_model.layers[:-31]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(num_classes, activation=\"softmax\")(x)    \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 24s 278ms/step - loss: 1.1344 - accuracy: 0.4980 - val_loss: 1.5019 - val_accuracy: 0.2862\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 18s 252ms/step - loss: 0.9914 - accuracy: 0.5629 - val_loss: 1.3836 - val_accuracy: 0.3002\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 18s 254ms/step - loss: 0.9202 - accuracy: 0.6173 - val_loss: 1.1741 - val_accuracy: 0.4188\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 19s 258ms/step - loss: 0.8912 - accuracy: 0.6243 - val_loss: 1.0933 - val_accuracy: 0.5236\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 19s 260ms/step - loss: 0.8193 - accuracy: 0.6622 - val_loss: 1.0952 - val_accuracy: 0.5602\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 19s 260ms/step - loss: 0.7941 - accuracy: 0.6626 - val_loss: 1.0679 - val_accuracy: 0.5689\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 18s 253ms/step - loss: 0.8037 - accuracy: 0.6565 - val_loss: 1.8302 - val_accuracy: 0.4241\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 18s 254ms/step - loss: 0.7580 - accuracy: 0.6848 - val_loss: 1.3466 - val_accuracy: 0.5462\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 19s 260ms/step - loss: 0.7855 - accuracy: 0.6678 - val_loss: 1.2938 - val_accuracy: 0.5515\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 18s 255ms/step - loss: 0.7540 - accuracy: 0.6844 - val_loss: 1.0055 - val_accuracy: 0.5742\n",
      "13/13 [==============================] - 2s 115ms/step - loss: 2.4750 - accuracy: 0.4086\n",
      "Test Accuracy: 0.4086\n"
     ]
    }
   ],
   "source": [
    "#ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "for layer in base_model.layers[:140]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 23s 255ms/step - loss: 1.3929 - accuracy: 0.2860 - val_loss: 1.4047 - val_accuracy: 0.2880\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 17s 240ms/step - loss: 1.3622 - accuracy: 0.2978 - val_loss: 1.3980 - val_accuracy: 0.2862\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 18s 242ms/step - loss: 1.3546 - accuracy: 0.3047 - val_loss: 1.3613 - val_accuracy: 0.2862\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 18s 243ms/step - loss: 1.3484 - accuracy: 0.2912 - val_loss: 1.3621 - val_accuracy: 0.2862\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 17s 239ms/step - loss: 1.3330 - accuracy: 0.2995 - val_loss: 1.4096 - val_accuracy: 0.2862\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 17s 240ms/step - loss: 1.3209 - accuracy: 0.3239 - val_loss: 1.3492 - val_accuracy: 0.2862\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 17s 240ms/step - loss: 1.3066 - accuracy: 0.3287 - val_loss: 1.4567 - val_accuracy: 0.3403\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 17s 240ms/step - loss: 1.3029 - accuracy: 0.3339 - val_loss: 1.8188 - val_accuracy: 0.1937\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 17s 241ms/step - loss: 1.2947 - accuracy: 0.3413 - val_loss: 2.6834 - val_accuracy: 0.1379\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 17s 241ms/step - loss: 1.2888 - accuracy: 0.3505 - val_loss: 1.3617 - val_accuracy: 0.3333\n",
      "13/13 [==============================] - 1s 65ms/step - loss: 1.5751 - accuracy: 0.2995\n",
      "Test Accuracy: 0.2995\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetB0\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
